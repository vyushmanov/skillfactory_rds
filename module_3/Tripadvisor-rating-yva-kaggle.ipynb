{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Образовательная платформа: SkillFactory\n",
    "##### Специализация: Data Science\n",
    "##### Группа: DST-17\n",
    "### Юнит 3. Проект: \"О вкусной и здоровой пище\"\n",
    "#### Выполнил: Владимир Юшманов\n",
    "\n",
    "![title](https://raw.githubusercontent.com/vyushmanov/skillfactory_rds/master/module_3/rating_pic.jpg)\n",
    "\n",
    "![title](https://raw.githubusercontent.com/vyushmanov/skillfactory_rds/master/module_3/TA_logo.JPG)\n",
    "\n",
    "\n",
    "# 1. Импорт и объединение данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'myfunction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e13138192a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Загружаем набор собственных функций\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmyfunction\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Сервисные функции\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'myfunction'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import os\n",
    "import re \n",
    "import plotly\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "%matplotlib inline\n",
    "\n",
    "# Загружаем специальный удобный инструмент для разделения датасета:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Загружаем набор собственных функций\n",
    "import myfunction as mf\n",
    "\n",
    "# Сервисные функции\n",
    "pd.set_option('display.max_rows', 50) # выведем больше строк\n",
    "pd.set_option('display.max_columns', 100) # выведем больше колонок\n",
    "import warnings; warnings.simplefilter('ignore') #  отключение вывода предупреждающих сообщений\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фиксируйте RANDOM_SEED и версию пакетов, чтобы эксперименты были воспроизводимы:\n",
    "RANDOM_SEED = 42\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/kaggle/input/sf-dst-restaurant-rating//main_task.csv' does not exist: b'/kaggle/input/sf-dst-restaurant-rating//main_task.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3e2f91d5cd8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Читаем датасеты\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/kaggle/input/sf-dst-restaurant-rating/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/main_task.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'kaggle_task.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/kaggle/input/sf-dst-restaurant-rating//main_task.csv' does not exist: b'/kaggle/input/sf-dst-restaurant-rating//main_task.csv'"
     ]
    }
   ],
   "source": [
    "# Читаем датасеты\n",
    "DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\n",
    "df_train = pd.read_csv(DATA_DIR + '/main_task.csv')\n",
    "df_test = pd.read_csv(DATA_DIR + 'kaggle_task.csv')\n",
    "sample_submission = pd.read_csv(DATA_DIR + '/sample_submission.csv')\n",
    "\n",
    "# ВАЖНО! для корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n",
    "\n",
    "# Выводим сводку о содержании датасета\n",
    "brief_columns = ['Признак', '#', 'тип данных', '% заполнения', '# пропусков', '# уникальных', 'диапазон значений / примеры']\n",
    "mf.brief_summary(df_train, brief_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Состав признаков. Описание:\n",
    "* __Restaurant_id__ — идентификационный номер ресторана / сети ресторанов\n",
    "* __City__ -- город, в котором находится ресторан\n",
    "* __Cuisine Style__ -- кухня или кухни, к которым можно отнести блюда, предлагаемые в ресторане\n",
    "* __Ranking__ -- место, которое занимает данный ресторан среди всех ресторанов своего города\n",
    "* __Price Range__ -- диапазон цен в ресторане в 3 категориях\n",
    "* __Number of Reviews__ -- количество отзывов о ресторане\n",
    "* __Reviews__ -- данные о двух последних отзывах, которые отображаются на сайте ресторана\n",
    "* __URL_TA__ -- страница ресторана на 'www.tripadvisor.com' \n",
    "* __ID_TA__ -- идентификатор ресторана в базе данных TripAdvisor\n",
    "* __Rating__ -- рейтинг ресторана по данным TripAdvisor (именно это значение должна будет предсказывать модель)\n",
    "\n",
    "Как видим, большинство признаков у нас требует очистки и предварительной обработки.\n",
    "\n",
    "Признак __ID_TA__ имеет количество уникальных значений меньшее, чем число строк датасета. Удалим дубликаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mf.drop_dublle(data, ['ID_TA', 'sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Разведывательный анализ данных (EDA)\n",
    "## 2.1. Restaurant_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод структуры уникальных и сетевых ресторанов\n",
    "# Добавление столбца с количеством ресторанов в сети\n",
    "    \n",
    "mf.view_count_in_chain(data)\n",
    "data = mf.calc_count_in_chain(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. City\n",
    "\n",
    ">### 2.2.1. Основной признак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строка преобразована в список\n",
    "data = mf.string_to_list_distribution(data, 'City')\n",
    "# выведем распределение ресторанов по городам, сохраним общую численность ресторанов в городе в признаке restorans_in_city \n",
    "data = mf.view_horiz_bar_n_table(data, 'City', 'restaurants_in_city') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">###  2.2.1. Дополнительные признаки, связанные с локализацией ресторанов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделены часто встречающиеся и редкие значения City\n",
    "\n",
    "# из внешних источников датасет дополнен сведениями city_is_the_capital, population_city, country\n",
    "data = mf.city_expansion_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Cuisine Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним пропуски значением 'Empty'\n",
    "# создадим признак empty_cuisine_style, в котором '1' обозначим пустые значения Cuisine Style, для остальных применим '0'\n",
    "\n",
    "# закодируем значения в переменной до их преобразования - признак code_cuisine_style\n",
    "\n",
    "# преобразуем строку с перечислением в список - признак list_cuisine_style\n",
    "\n",
    "# выделим редко встречающиеся кухни, обозначим unique_style. Считаем, что редкие кухни - последние 23 (встречаются реже 20 раз)\n",
    "\n",
    "# посчитаем количество заявленных кухонь - признак count_cuisine_style. В случае отсутствия информации\n",
    "# о количестве кухонь используем медианное значение\n",
    "# выведем гистограммы и боксплоты для линейных и логарифмических значений признака\n",
    "\n",
    "# добавим признак, рассчитанный как натуральный логарифм количества кухонь\n",
    "    \n",
    "# выделим редко встречающиеся кухни. Последнее количество unique_border - уникальные кухни.\n",
    "data = mf.string_to_list_distribution(data, 'Cuisine Style')\n",
    "data = mf.cuisine_distribution(data, 'list_cuisine_style')\n",
    "data = mf.rife_rare_distribution(data, 'list_cuisine_style',.2,.03)\n",
    "data = mf.localisation_cuisine_country(data) # идею позаимствовал у (с)Rezinko Mikhail\n",
    "data = mf.view_histogrm_n_boxplot(data, 'count_cuisine_style')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определение наиболее частых и наиболее редких вариантов признака\n",
    "data = mf.rife_rare_distribution(data, 'list_cuisine_style', .25, .02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на распределение признака в 10 крупнейших городах:\n",
    "mf.view_attribute_based_distribution(data, 'Ranking', 'City', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# произведем сквозное ранжирование равномерно распределив ранг ресторанов в рамках города\n",
    "# значение нового признака сохраним в total_ranking\n",
    "\n",
    "# в качестве альтернативного способа преобразования признака используем стандартизацию и сохраняем результат в standard_ranking\n",
    "\n",
    "data = mf.ranking_distribution(data)\n",
    "data = mf.add_ranking_distribution(data)\n",
    "\n",
    "mf.view_attribute_based_distribution(data, 'total_ranking', 'City', 10)\n",
    "mf.view_attribute_based_distribution(data, 'standard_ranking', 'City', 10)\n",
    "mf.view_attribute_based_distribution(data, 'norm_ranking_on_population', 'City', 10)\n",
    "mf.view_attribute_based_distribution(data, 'norm_ranking_on_tourists', 'City', 10)\n",
    "mf.view_attribute_based_distribution(data, 'norm_ranking_on_max_rank', 'City', 10)\n",
    "mf.view_attribute_based_distribution(data, 'norm_ranking_on_restaurant', 'City', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные признаки проверим на коррекляцию с ключевой переменной, после чего примем решение о их использовании в модели. Предполагаю, что лучший результат покажет признак standard_ranking\n",
    "## 2.5. Rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.6. Price Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим признак empty_price_range, в котором '1' обозначим пустые значения Price Range, для остальных применим '0'\n",
    "\n",
    "# перекодируем признак по словарю {'$':1, '$$ - $$$':2, '$$$$':3}\n",
    "\n",
    "# заполним пропущенные значения модой, т.е. 2\n",
    "\n",
    "data = mf.price_distribution(data, 2)\n",
    "data = mf.mean_price_in_city(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Number of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приведем наименование столбца к стандартному виду\n",
    "data.rename(columns={'Number of Reviews': 'number_of_reviews'}, inplace=True)\n",
    "\n",
    "# зафиксируем строки с пустыми значениями\n",
    "data['empty_number_of_reviews'] = pd.isna(data['number_of_reviews']).astype('float64')\n",
    "\n",
    "# вывод гистограмм и таблицы\n",
    "data = mf.view_histogrm_n_boxplot(data, 'number_of_reviews')\n",
    "# добавлен признак, рассчитанный как натуральный логарифм номера ревю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исследуем вляние различных признаков на распределение log_number_of_reviews:\n",
    "mf.view_attribute_based_distribution(data, 'log_number_of_reviews', 'City', 5)\n",
    "mf.view_attribute_based_distribution(data, 'log_number_of_reviews', 'Price Range', 4)\n",
    "mf.view_attribute_based_distribution(data, 'log_number_of_reviews', 'count_cuisine_style', 8)\n",
    "\n",
    "# в целях детального изучения распределения вновь созданного признака log_number_of_reviews выведем подробную гистограмму и расчет выбросов \n",
    "mf.view_histogram_n_outliers(data, 'number_of_reviews', 'log', 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбросы зафиксированы в 10 наблюдениях, удалим их, предварительно сохранив информацию о них\n",
    "data['outliers_number_of_reviews'] = pd.DataFrame(data['log_number_of_reviews'] > 8.56).astype('float64')\n",
    "data.loc[data['log_number_of_reviews'] > 8.56, 'number_of_reviews']=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report from Number of Reviews\n",
    "> При исследовании влияния дополнительных признаков на распределение логарифма номера ревю установлено, что в более ранних выпусках ревю гораздо чаще встречается отсутствие упоминания о ценовой категории ресторана (Price Range).\n",
    "Остальные исследованные признаки не имеют выраженного влияния на распределение логарифмического признака номера ревю.\n",
    "\n",
    "> Выявлены и удалены 10 выбросов\n",
    "## 2.8. Reviews\n",
    "> ### 2.8.1. Обработка содержания отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строковая переменная преобразована в словари используемых в отзывах слов, которые сохранены в признак review_words_list\n",
    "# строки с пустыми отзывами отмечены 1 в признаке empty_review\n",
    "\n",
    "# произведен подсчет слов, имеющих позитивную и негативную окраску. Количество таких слов сохранено в признаки count_pos_words\n",
    "# и count_neg_words соответственно. В случае присутствия в отзывах слова 'not' из количества негативных слов вычитается 1.\n",
    "\n",
    "data = mf.review_text_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Работа с текстовой информацией признака Reviews завершена. Позднее будет произведено преобразование признака review_words_list, содержащего списки используемых в отзывах слов в dummy-переменную.\n",
    "> ### 2.8.2 Обработка информации о датах размещения отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим из Reviews информацию о датах размещения отзывов\n",
    "# рассчитаем количество дней, прошедших между публикациями отзывов\n",
    "# определим для каждого ресторана, положение самого свежего отзыва на временном луче, нулевая отметка которого соответствует \n",
    "# дню выхода самой первой публикации \n",
    "data = mf.data_review_distribution(data)\n",
    "\n",
    "# произведем сравнение распределений количества дней после новейшей публикации и периодом между публикациями\n",
    "fig = px.scatter(data[data['review_date_count'] == 2], x=\"review_date_min\", y=\"review_date_delta\",\n",
    "                marginal_x='histogram', marginal_y='histogram',\n",
    "                trendline='ols', trendline_color_override='darkblue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mf.view_histogrm_n_boxplot(data, 'review_date_min')\n",
    "data = mf.view_histogrm_n_boxplot(data, 'review_date_delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исследуем распределение признаков review_date_olded и review_date_delta    \n",
    "mf.view_histogram_n_outliers(data, 'review_date_min', 'all', 200)\n",
    "mf.view_histogram_n_outliers(data, 'review_date_delta', 'lin', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Корреляционный анализ признаков\n",
    "> ### 2.9.1. Построение матрицы корреляций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# бесконечности и пропуски заменены\n",
    "data.replace(np.inf, 1, inplace=True)\n",
    "data.replace(-np.inf, 0, inplace=True)\n",
    "data = data.fillna(0)\n",
    "\n",
    "mf.show_heatmap(data[data['sample'] == 1].drop(['sample'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.9.2. Оптимизация признаков скоррелированных между собой и признаков не имеющих корреляции с целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Из признаков, полученных вокруг Ranking оставляем standard_ranking как имеющий наибольшую корреляцию\n",
    "data.drop(['total_ranking'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Из пар линейного значения и логарифма выбираем имеющие наибольшую корреляцию, остальные удаляем\n",
    "data.drop(['log_count_cuisine_style', 'log_number_of_reviews', 'log_review_date_min', 'log_review_date_delta'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Признак outliers_number_of_reviews не имеет корреляции с целевой переменной - удаляем\n",
    "data.drop(['outliers_number_of_reviews'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# произведение модулей признаков empty_review и review_date_count, первичные признаки удалены\n",
    "#data['empty_review_date_count'] = data['empty_review'] + data['review_date_count']\n",
    "#data.drop(['empty_review'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "mf.show_heatmap(data[data['sample'] == 1].drop(['sample'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.9.3. Применение метода главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_for_pca = ['empty_review', 'review_date_count', 'empty_number_of_reviews', 'count_review_words', 'code_review_words', 'number_of_reviews']\n",
    "#data = mf.pca_distribution(data, list_for_pca, 'pca_review')\n",
    "\n",
    "#list_for_pca = ['code_city', 'restaurants_in_city', 'population_city', 'code_country', 'count_city_tourists', 'count_in_chain', 'city_is_the_capital']\n",
    "#data = mf.pca_distribution(data, list_for_pca, 'pca_city', [0,1,1,0,0,1,0])\n",
    "\n",
    "#list_for_pca = ['code_cuisine_style', 'empty_cuisine_style', 'rare_cuisine_style', 'local_cuisine']\n",
    "#data = mf.pca_distribution(data, list_for_pca, 'pca_cuisine')\n",
    "\n",
    "list_for_pca = ['Ranking', 'count_in_chain', 'standard_ranking', 'restaurants_in_city', 'population_city']\n",
    "data = mf.pca_distribution(data, list_for_pca, 'pca_ranking', [1,0,1,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Создание дополнительных признаков (генерация фичей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество ресторанов на 1000 человек населения города, первичные признаки удалены\n",
    "#data['restaurant_on_population'] = data['restaurants_in_city'] / data['population_city'] / 1000\n",
    "\n",
    "# число туристов на одного жителя города\n",
    "#data['tourists_on_population'] = data['count_city_tourists'] / data['population_city']\n",
    "\n",
    "# частное от деления Ranking на количество ресторанов в городе (restaurants_in_city)\n",
    "#data['ranking_on_count_restaurant'] = data['Ranking'] / data['restaurants_in_city']\n",
    "\n",
    "# количество отзывов на 10 000 жителей\n",
    "#data['review_on_population'] = data['number_of_reviews'] / data['population_city'] / 10000\n",
    "\n",
    "# количество отзывов на 100 000 туристов\n",
    "#data['review_on_tourists'] = data['number_of_reviews'] / data['population_city'] / 100000\n",
    "\n",
    "# отношение количества туристов к жителям\n",
    "#data['tourists_on_population'] = data['count_city_tourists'] / data['population_city']\n",
    "\n",
    "#data.drop(['city_is_the_capital', 'code_cuisine_style', 'local_cuisine', 'empty_number_of_reviews',\n",
    "#          'review_date_count', 'empty_review_date_count', 'count_neg_words'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "#mf.show_heatmap(data[data['sample'] == 1].drop(['sample'], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. Преобразование признаков в dummy-переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mf.prep_dummies(data, 'name_chain', 1, 'ch_')#\n",
    "#data = mf.prep_dummies(data, 'list_city')\n",
    "#data = mf.prep_dummies(data, 'list_country')\n",
    "#data = mf.prep_dummies(data, 'list_cuisine_style', .85, 'cs_')\n",
    "#positive_words, negative_words = mf.read_positive_words()\n",
    "#data = mf.prep_dummies(data, 'list_review_words', .3, 'w_', positive_words) # обработаны только слова из \"позитивного списка\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Report Разведывательный анализ данных (EDA)\n",
    "> Испольование полного набора преобразованных и дополненных данных приводит к катастрофическому переобучению. В целях выявления оптимального набора признаков, обеспечивающих наилучшие условия для адекватного обучения модели, произведено поэлементное добавление признаков с оценкой влияния каждого признака на результат (сравнение MAE) и проверкой на переобучение.\n",
    "\n",
    "# 3. Предподготовка данных (Data Preprocessing)\n",
    "## 3.1. Процедуры преобразования данных\n",
    "Для удобства маневрирования методами преобразования данных, весь код, производящий предподготовку, локализован в этой ячейке. При работе с данными в настоящей ячеке не отображаются диаграммы и таблицы, что сокращает время обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mf.read_dataframes() # чтение файлов и формирование исходного датасета\n",
    "data = mf.drop_dublle(data, ['ID_TA', 'sample']) # удаление дублей\n",
    "data.rename(columns={'Number of Reviews': 'number_of_reviews'}, inplace=True)\n",
    "data['empty_number_of_reviews'] = pd.isna(data['number_of_reviews']).astype('float64')\n",
    "data = mf.string_to_list_distribution(data, 'City') # строка преобразована в список 'list_city'\n",
    "\n",
    "data = mf.ranking_distribution(data)\n",
    "data = mf.city_expansion_features(data)\n",
    "data = mf.add_ranking_distribution(data)\n",
    "list_for_pca = ['total_ranking', 'standard_ranking', 'norm_ranking_on_max_rank', \n",
    "                'norm_ranking_on_restaurant', 'norm_ranking_on_population', 'norm_ranking_on_tourists']\n",
    "data = mf.pca_distribution(data, list_for_pca, 'pca_norm_ranking', [1,1,1,1,1,1])\n",
    "\n",
    "data['log_number_of_reviews'] = np.log1p(data['number_of_reviews'])\n",
    "\n",
    "data = mf.calc_count_in_chain(data)\n",
    "data = mf.prep_dummies(data, 'name_chain', 1, 'ch_') # преобразование признаков в dummy-переменные\n",
    "data = mf.prep_dummies(data, 'list_city') # преобразование признаков в dummy-переменные\n",
    "data = mf.review_text_distribution(data)\n",
    "positive_words, negative_words = mf.read_positive_words()\n",
    "data = mf.prep_dummies(data, 'list_review_words', .3, 'w_', positive_words) # обработаны только слова из \"позитивного списка\"\n",
    "\n",
    "data = mf.price_distribution(data, 2)\n",
    "data = mf.mean_price_in_city(data)\n",
    "\n",
    "data = mf.string_to_list_distribution(data, 'Cuisine Style')\n",
    "data = mf.cuisine_distribution(data, 'list_cuisine_style')\n",
    "#data = mf.rife_rare_distribution(data, 'list_cuisine_style',.3,.01)\n",
    "#data = mf.city_expansion_features(data)\n",
    "#data = mf.localisation_cuisine_country(data)\n",
    "#data = mf.prep_dummies(data, 'list_cuisine_style', .85, 'cs_')\n",
    "\n",
    "data = mf.data_review_distribution(data)\n",
    "\n",
    "#data['outliers_date_min'] = pd.DataFrame(data['review_date_min'] > 1122.5).astype('float64')\n",
    "#data.loc[data['review_date_min'] > 1122.5, 'number_of_reviews']=None\n",
    "#data['outliers_date_delta'] = pd.DataFrame(data['review_date_delta'] > 355.5).astype('float64')\n",
    "#data.loc[data['review_date_delta'] > 355.5, 'number_of_reviews']=None\n",
    "\n",
    "#data['empty_number_of_reviews'] = pd.isna(data['number_of_reviews']).astype('float64')\n",
    "\n",
    "#data = mf.ranking_distribution(data)\n",
    "list_for_pca = ['Ranking', 'standard_ranking', 'total_ranking']\n",
    "data = mf.pca_distribution(data, list_for_pca, 'pca_ranking',[1,1,1])\n",
    "data['ranking_power'] = data['Ranking']* data['Ranking']\n",
    "data['ranking_copy'] = data['Ranking']\n",
    "\n",
    "#data['log_number_of_reviews'] = np.log1p(data['number_of_reviews'])\n",
    "#data['log_review_date_min'] = np.log1p(data['review_date_min'])\n",
    "#data['log_review_date_delta'] = np.log1p(data['review_date_delta'])\n",
    "\n",
    "#data['outliers_number_of_reviews'] = pd.DataFrame(data['log_number_of_reviews'] > 8.56).astype('float64')\n",
    "#data.loc[data['log_number_of_reviews'] > 8.56, 'number_of_reviews']=None\n",
    "\n",
    "data.drop(['count_in_chain', 'code_review_words',\n",
    "           'count_pos_words', 'count_neg_words'], axis=1, inplace=True, errors='ignore')\n",
    "data.drop(['total_ranking', 'standard_ranking'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Стандартизация и удаление нечисловых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)\n",
    "# произведена мин-макс стандартизация (за исключением списка столбцов)\n",
    "data = mf.normalisation(data, MinMaxScaler(), ['Rating', 'sample'])\n",
    "\n",
    "df_preproc = mf.delete_string_sign(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Поэлементный контроль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_preproc.sample(2))\n",
    "display(data.describe().head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Выделение тестовой части датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь выделим тестовую часть\n",
    "\n",
    "train_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "y = train_data[['Rating']]           # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)\n",
    "\n",
    "# Воспользуемся специальной функцией train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model \n",
    "## 4.1. Тестовое обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки:\n",
    "from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\n",
    "from sklearn import metrics # инструменты для оценки точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\n",
    "model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n",
    "\n",
    "# Обучаем модель на тестовом наборе данных\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n",
    "# Предсказанные значения записываем в переменную y_pred\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.array([5.0 if x>5 else x for x in list(np.round(y_pred * 2) / 2)])\n",
    "\n",
    "# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n",
    "# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "try: title = 'MAE: '+str(MAE)+' <-- '+str(old_MAE)\n",
    "except: title = 'MAE: '+str(MAE)\n",
    "old_MAE = MAE\n",
    "\n",
    "layout =go.Layout(\n",
    "              autosize=False,\n",
    "              width=1000,\n",
    "              height=500)\n",
    "fig = go.Figure(layout = layout)\n",
    "fig.add_trace(go.Bar(x = model.feature_importances_, y = X.columns, orientation='h')), \n",
    "fig.update_layout(title = title, title_x = 0.5,\n",
    "                  yaxis={'categoryorder':'total descending'},\n",
    "                  margin = dict(l=200, r=100, t=50, b=0), showlegend=False)\n",
    "fig.update_yaxes(range=(-.5, 20.5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Выбор оптимального набора признаков\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# блок тестирования оптимального набора признаков\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "list_importance_sign = list(feat_importances.nlargest(len(train_data.columns)-1).index)\n",
    "min_MAE = round(MAE,3)\n",
    "print(f\"min_MAE = {min_MAE}\")\n",
    "remove_list = []\n",
    "log = []\n",
    "delta =0.001\n",
    "for i in range(0,len(list_importance_sign),1):\n",
    "    col = list_importance_sign[i]\n",
    "    print(f\"{i}.{col}\")\n",
    "#     ###\n",
    "    train_data = data.query('sample == 1').drop(['sample'], axis=1)\n",
    "    test_data = data.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "    y = train_data.Rating.values            # наш таргет\n",
    "    X = train_data.drop(['Rating']+[col], axis=1)\n",
    "\n",
    "    # Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "    # выделим 20% данных на валидацию (параметр test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    print(test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred = np.array([5.0 if x>5 else x for x in list(np.round(y_pred * 2) / 2)])\n",
    "    temp_MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "#     ###\n",
    "    print(temp_MAE)\n",
    "    log.append([col, temp_MAE])\n",
    "    if round(temp_MAE,3) <= min_MAE-delta:\n",
    "        remove_list.append(col)\n",
    "        print(f\"удаляем:= {col}\")\n",
    "    else:\n",
    "        print(f\"не удаляем:= {col}\")\n",
    "print(f\"i={i}\")\n",
    "print(f\"remove_list: {remove_list}\")\n",
    "print(f\"log_list: {log}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Обучение на оптимальном наборе признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Если все устраивает - готовим Submission на kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['Rating'], axis=1)\n",
    "predict_submission = model.predict(test_data)\n",
    "predict_submission = np.array([5.0 if x>5 else x for x in list(np.round(predict_submission * 2) / 2)])\n",
    "\n",
    "sample_submission['Rating'] = predict_submission\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # What's next?\n",
    "Или что делать, чтоб улучшить результат:\n",
    "* Обработать оставшиеся признаки в понятный для машины формат\n",
    "* Посмотреть, что еще можно извлечь из признаков\n",
    "* Сгенерировать новые признаки\n",
    "* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n",
    "* Подобрать состав признаков\n",
    "\n",
    "В общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
