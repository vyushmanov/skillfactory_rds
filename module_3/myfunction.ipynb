{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import os\n",
    "import re \n",
    "import plotly\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from tabulate import tabulate\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "\n",
    "# Загружаем специальный удобный инструмент для разделения датасета:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# %% [code]\n",
    "\n",
    "def drop_dublle(data, columns):\n",
    "    data.drop_duplicates(subset=columns, keep='first', inplace=True)\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После удаления дублирующих строк по идентификатору '{}'\".\n",
    "                format(columns))\n",
    "    return data\n",
    "\n",
    "def season_from_year(date):\n",
    "    if date == 0:\n",
    "        return 'empty'\n",
    "    else:\n",
    "        month = date.month\n",
    "        if month == 12 or month < 3:\n",
    "            return ['winter']\n",
    "        elif month < 6:\n",
    "            return ['spring']\n",
    "        elif month < 9:\n",
    "            return ['summer']\n",
    "        else:\n",
    "            return ['autumn']\n",
    "    \n",
    "def print_report(rows_total, rows_train, columns, text):\n",
    "    print(text +', Датасет содержит признаков - {}; строк - {}, из них {} - train.'.\n",
    "         format(columns, rows_total, rows_train))\n",
    "\n",
    "# соберем из списка списков одноуровневый список\n",
    "def list_extend(list_of_lists):\n",
    "    result=[]\n",
    "    for lst in list_of_lists:\n",
    "        result.extend(lst)\n",
    "    return result\n",
    "\n",
    "# вывод таблицы со сводкой о датасете\n",
    "def brief_summary(data, brief_columns):\n",
    "    df = pd.DataFrame(columns = brief_columns)\n",
    "    columns_list = data.columns.to_list()\n",
    "    for x in range(len(columns_list)):\n",
    "        column = columns_list[x]\n",
    "        count = len(data[data[column].isnull()])\n",
    "        res = str(data[column].iloc[0])+'<br>'+str(data[column].iloc[1])+'<br>'+str(data[column].iloc[2])\n",
    "        df.loc[x] = [\n",
    "            '<b>'+column+'</b>',\n",
    "            len(data[column]),\n",
    "            str(data[column].dtype),\n",
    "            round((1 - len(data[data[column].isnull()])/len(data))*100, 1),\n",
    "            count,\n",
    "            data[column].nunique(),\n",
    "            res\n",
    "        ]\n",
    "    \n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        columnwidth = [65,50,50,65,60,65,350],\n",
    "            header=dict(values=brief_columns,\n",
    "                        fill_color='paleturquoise',\n",
    "                        align='center',\n",
    "                       font=dict(size=12)),\n",
    "            cells=dict(values=[df['Признак'], df['#'], df['тип данных'], df['% заполнения'],\n",
    "                            df['# пропусков'], df['# уникальных'], df['диапазон значений / примеры']],\n",
    "                       fill_color='lavender',\n",
    "                       align=['left'] + ['center']*5 + ['left'],\n",
    "                      height=60))\n",
    "        ])\n",
    "    fig.update_layout(margin = dict(l=50, r=50, t=50, b=20))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Добавление столбца с количеством ресторанов в сети\n",
    "def calc_count_in_chain(data):    \n",
    "    df = pd.DataFrame(data['Restaurant_id'].value_counts()).reset_index()\n",
    "    df.columns = ['Restaurant_id', 'count_in_chain']\n",
    "    if 'count_in_chain' in data.columns.to_list():\n",
    "        data.drop(['count_in_chain'], axis=1, inplace = True)\n",
    "    data = pd.merge(data, df, on = 'Restaurant_id')\n",
    "    data['name_chain'] = data['count_in_chain'].apply(lambda x: ['single'] if x<2 else ['several'] if x<5 else ['many'])\n",
    "    text = ('Добавлены признаки count_in_chain с количеством ресторанов в объединении,'\n",
    "            'к которому имеет отношение текущий ресторан и признак сети/объединения - code_chain')\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), text)\n",
    "    return data\n",
    "    \n",
    "# вывод структуры уникальных и сетевых ресторанов \n",
    "def view_count_in_chain(data):\n",
    "    df = data['Restaurant_id'].value_counts()\n",
    "    layout = go.Layout(\n",
    "          autosize=False,\n",
    "          width=1200,\n",
    "          height=350)\n",
    "    fig = go.Figure(layout = layout)\n",
    "    fig.add_trace(go.Histogram(x = df[df == 1], opacity=.75, name = 'уникальные рестораны'))\n",
    "    fig.add_trace(go.Histogram(x = df[(df > 1) & (df <= 4)], opacity=.75, name = 'ресторанные объединения'))\n",
    "    fig.add_trace(go.Histogram(x = df[df > 4], opacity=.75, name = 'ресторанные сети'))\n",
    "    fig.update_layout(title = 'Структура рынка уникальных и сетевых ресторанов',\n",
    "                     title_x = 0.5,\n",
    "                     xaxis_title = 'Ресторанов в объединении',\n",
    "                     yaxis_title = '# ресторанов / объединений / сетей',\n",
    "                     legend = dict(x = .8, y = 0.84,xanchor = 'center', orientation = 'v'),\n",
    "                     #barmode = 'overlay',\n",
    "                     margin = dict(l=100, r=50, t=50, b=20))\n",
    "    fig.show()\n",
    "\n",
    "# преобразование строк в списки\n",
    "def string_to_list_distribution(data, column, new_column=True, empty_value=[]):\n",
    "    # замена пропусков значением empty\n",
    "    empty_values = [None, np.nan, 'nan']\n",
    "    empty_values.append(empty_value)\n",
    "    try: data[column].fillna('empty')\n",
    "    except: a = 1\n",
    "    data[column] = data[column].map(lambda x: 'empty' if x in empty_values else x)\n",
    "\n",
    "    data['temp'] = data[column].apply(lambda x: \"'\"+str(x)+\"'\") # сервисный столбец\n",
    "    \n",
    "    # кодирование исходной переменной после добавления empty\n",
    "    code_column = 'code_'+str(column).replace(' ', '_').lower()\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data['temp'])\n",
    "    data[code_column] = le.transform(data['temp'])\n",
    "    \n",
    "    # преобразование в список\n",
    "    if new_column == True:\n",
    "        new_column = 'list_'+str(column).replace(' ', '_').lower()\n",
    "    else: new_column = column\n",
    "    data[new_column] = data['temp'].str.findall(r\"'(\\b.*?\\b)'\")\n",
    "    data.drop(['temp'], inplace=True, axis=1)  \n",
    "    \n",
    "    print(\"Строковый признак '{}' преобразован в список и сохранен в столбец '{}'\".format(column, new_column))\n",
    "    return data    \n",
    "\n",
    "# преобразования признака Cuisine Style\n",
    "def cuisine_distribution(data, column):\n",
    "    # зафиксировать пустые значения в отдельной переменной\n",
    "    data['empty_cuisine_style'] = data[column].apply(lambda x: 1 if 'empty' in x else 0)\n",
    "    \n",
    "    # посчитаем количество заявленных кухонь - признак count_cuisine_style\n",
    "    data['count_cuisine_style'] = data[column].apply(lambda x: len(x)).astype('float64')\n",
    "\n",
    "    # пропуски в столбце count_cuisine_style средним значением\n",
    "    median = np.median(data[data['empty_cuisine_style'] != 1]['count_cuisine_style'])\n",
    "    data['count_cuisine_style'] = data[column].apply(lambda x: median if 'empty' in x else len(x))\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признаков вокруг Cuisine Style\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# определение наиболее частых и наиболее редких вариантов признака\n",
    "def rife_rare_distribution(data, column, high_percent=0.3, low_percent=0.02):\n",
    "    temp_list = data[column].tolist()\n",
    "    sign_set = Counter(list_extend(temp_list)).most_common()\n",
    "    sign_df = pd.DataFrame(sign_set, columns=[column, '#'])\n",
    "    \n",
    "    high_count = sign_df['#'].sum() * high_percent\n",
    "    low_count = sign_df['#'].sum() * low_percent\n",
    "    \n",
    "    high_list = []\n",
    "    sum_count = 0\n",
    "    for i in range(len(sign_df['#'])):\n",
    "        if sum_count < high_count:\n",
    "            if sign_df[column][i] != 'empty':\n",
    "                sum_count += sign_df['#'][i]\n",
    "                high_list.append(sign_df[column][i])\n",
    "        else: break\n",
    "    \n",
    "    low_list = []\n",
    "    sum_count = 0\n",
    "    for i in range(len(sign_df['#']))[::-1]:\n",
    "        if sum_count < low_count:\n",
    "            if sign_df[column][i] != 'empty':\n",
    "                sum_count += sign_df['#'][i]\n",
    "                low_list.append(sign_df[column][i])\n",
    "        else: break\n",
    "        \n",
    "    data[column.replace('list_', 'rife_')] = data.apply(lambda x: 1 if len(set(x[column])&set(high_list)) > 0 and\n",
    "                                                        x[column.replace('list_', 'empty_')] != 1 else 0, axis=1)    \n",
    "    data[column.replace('list_', 'rare_')] = data[column].apply(lambda x: 1 if len(set(x)&set(low_list)) > 0 else 0)\n",
    "    text = (\"Признак частого повторения присвоен {} вариантам в {} строках. \"\n",
    "            \"\\nПризнак редкого использования присвоен {} вариантам в {} строках\").format(len(high_list), \n",
    "                                                                                       len(data[data[column.replace('list_', 'rife_')] == 1]),\n",
    "                                                                                       len(low_list),\n",
    "                                                                                       len(data[data[column.replace('list_', 'rare_')] == 1]))\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), text+\"\\nПосле добавления признаков наиболее частых и редких {}\".\n",
    "                format(column.replace('list_', '')))    \n",
    "    return data\n",
    "\n",
    "\n",
    "def localisation_cuisine_country(data):\n",
    "    dict_cuisine_by_country = {\n",
    "        'United Kingdom':['British','Scottish'],\n",
    "        'Spain': ['Spanish', 'Mediterranean', 'Latin'],\n",
    "        'France': ['French','Central European', 'Mediterranean'], \n",
    "        'Italy': ['Italian','Central European', 'Mediterranean', 'Latin'],\n",
    "        'Germany': ['Dutch','German','Central European'],\n",
    "        'Portugal': ['Portuguese', 'Latin'],\n",
    "        'Czechia': ['Czech','Eastern European'],\n",
    "        'Poland':['Polish','Eastern European'],\n",
    "        'Austria': ['Austrian','Central European'],\n",
    "        'Netherlands':['Central European'],\n",
    "        'Belgium': ['Belgian','Eastern European'],\n",
    "        'Switzerland':['Swiss','Central European'],\n",
    "        'Sweden':['Scandinavian', 'Balti'],\n",
    "        'Hungary':['Hungarian','Eastern European'],\n",
    "        'Ireland':['Irish'],\n",
    "        'Denmark':['Danish', 'Balti'],\n",
    "        'Greece':['Greece', 'Mediterranean'],\n",
    "        'Norway':['Scandinavian','Balti'],\n",
    "        'Finland':['Scandinavian','Balti'],\n",
    "        'Slovakia':['Eastern European'],\n",
    "        'Luxembourg':['Eastern European'],\n",
    "        'Slovenia':['Slovenian','Eastern European']\n",
    "    }\n",
    "    data['local_cuisine'] = data.apply(lambda x: 1 if len(set(x['list_cuisine_style'])&set(dict_cuisine_by_country[x['country']])) > 0 else 0, axis=1)\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признака соответствия кухни ресторана региону его локализации\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Работа с категориальной переменной\n",
    "def prep_dummies(data, column, percent=1, prefix='', limit_list = []):\n",
    "    temp_list = data[column].tolist()\n",
    "    sign_set = Counter(list_extend(temp_list)).most_common()\n",
    "    sign_df = pd.DataFrame(sign_set, columns=[column, '#'])\n",
    "    \n",
    "    high_count = sign_df['#'].sum() * percent\n",
    "    \n",
    "    target_list = []\n",
    "    sum_count = 0\n",
    "    for i in range(len(sign_df['#'])):\n",
    "        if sum_count < high_count:\n",
    "            if sign_df[column][i] != 'empty':\n",
    "                sum_count += sign_df['#'][i]\n",
    "                target_list.append(sign_df[column][i])\n",
    "        else: break\n",
    "            \n",
    "    target_drop = target_list\n",
    "       \n",
    "    if percent < 1:\n",
    "        text = (\"При подготовке признака, из перечня переменных использован {}-й процентиль. \"\n",
    "                 \"Из первичного списка в {} значений оставлены {}\".\n",
    "                format(int(sum_count/sign_df['#'].sum()*100), len(sign_set), len(target_list)))\n",
    "    else: text = ''\n",
    "        \n",
    "    if len(limit_list) > 0:\n",
    "        target_list = set(target_list)&set(limit_list)\n",
    "        text = (\"При подготовке признака, из {} значений оставлены {}\".\n",
    "                format(len(target_drop), len(target_list)))\n",
    "        \n",
    "    for col in target_list:\n",
    "        if col != 'empty':\n",
    "#            if percent < 1:\n",
    "#                data[column.replace('list_', 'other_')] = data[column].apply(lambda x: 1\n",
    "#                                                                             if len(set(col)&(set(sign_df[column].to_list()) - set(target_list))) > 0 \n",
    "#                                                                             else 0)\n",
    "            data[prefix+col] = data[column].apply(lambda x: 1 if col in x else 0)\n",
    "\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), text+\"\\nПосле преобразования {} в dummy-переменную\".\n",
    "                format(column.replace('list_', '')))    \n",
    "    return data\n",
    "\n",
    "# для Столбца: вывод на экран горизонтальной диаграммы и таблицы с количеством значений\n",
    "def view_horiz_bar_n_table(data, column, new_sign='no'):\n",
    "    # добавление признака с количеством объектов column\n",
    "    list_counter = Counter(data[column])\n",
    "    if new_sign !='no':\n",
    "        data[new_sign] = data[column].apply(lambda x: list_counter[x])\n",
    "    \n",
    "    list_counter = list_counter.most_common()\n",
    "    reversed_list_counter = list_counter[::-1]\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, column_widths=[1000,400], specs=[[{\"type\": \"bar\"}, {\"type\": \"table\"}]])\n",
    "\n",
    "    trace0 = go.Bar(y = [x[0] for x in reversed_list_counter], x = [x[1] for x in reversed_list_counter], orientation='h')\n",
    "\n",
    "    trace1 = go.Table(header=dict(values=[column, '#'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='center'),\n",
    "                    cells=dict(values=[[x[0] for x in list_counter], [x[1] for x in list_counter]],\n",
    "                    fill_color='lavender',\n",
    "                    align='center'))\n",
    "\n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 1, 2)\n",
    "    \n",
    "    fig.update_layout(margin = dict(l=100, r=50, t=20, b=0))\n",
    "\n",
    "    fig.show()\n",
    "    return data\n",
    "\n",
    "# расширение признаков вокруг City с использованием внешних источников данных    \n",
    "def city_expansion_features(data):\n",
    "    df_city = pd.read_csv('/kaggle/input/world-cities-datasets/'+'/worldcities.csv')\n",
    "    \n",
    "    data['City'] = data['City'].map(lambda x: 'Porto' if x == 'Oporto' else x)\n",
    "    list_counter = Counter(data['City'])\n",
    "    city_info = pd.DataFrame(columns=['City', 'city_is_the_capital', 'population_city', 'country'])\n",
    "    for index, city in enumerate(list_counter.keys()):\n",
    "        df = df_city[df_city['city_ascii'] == city].iloc[0]\n",
    "        city_info.loc[index] = [\n",
    "            city,\n",
    "            1 if df['capital'] == 'primary' else 0,\n",
    "            int(df['population']),\n",
    "            df['country']\n",
    "        ]\n",
    "    \n",
    "    city_info = string_to_list_distribution(city_info, 'country')\n",
    "    \n",
    "    data = pd.merge(data, city_info, on = 'City')\n",
    "    \n",
    "    df_city_tourists = pd.read_excel('/kaggle/input/tourists-in-europe-city/'+'/city of Europe.xlsx')\n",
    "    df_city_tourists = df_city_tourists[['City', 'Tourists']]\n",
    "    df_city_tourists.columns = ['City', 'count_city_tourists']\n",
    "    \n",
    "    data = pd.merge(data, df_city_tourists, on = 'City')\n",
    "    \n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признаков вокруг City\")\n",
    "    return data\n",
    "\n",
    "# вывод группы из четырех диаграмм и таблицы (для числовых признаков)\n",
    "def view_histogrm_n_boxplot(data, column):\n",
    "    ds = data[data['sample'] == 1][column]\n",
    "    ds = ds[ds != 0]\n",
    "    dsl = np.log1p(ds)\n",
    "    data['log_'+column] = np.log1p(data[column])\n",
    "    data['log_'+column] = data['log_'+column].map(lambda x: np.nan if x == float('-inf') else x)\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=5, specs=[[{\"type\": \"histogram\"}, {\"type\": \"box\"}, {\"type\": \"histogram\"}, {\"type\": \"box\"}, {\"type\": \"table\"}]])\n",
    "\n",
    "    trace0 = go.Histogram(x = ds, opacity=.65)\n",
    "    count_cuisine_linear = go.Box(y = ds, marker_color = 'black', opacity=.5)\n",
    "    trace2 = go.Histogram(x = dsl, opacity=.65, nbinsx = 8)\n",
    "    count_cuisine_log = go.Box(y = dsl, marker_color = 'black', opacity=.5)\n",
    "    \n",
    "    sign_name = ['строк', 'тип', 'значений', 'пропусков', 'min', 'max', 'mean',' median']\n",
    "    values = [\n",
    "        len(data[data['sample'] == 1]),\n",
    "        str(ds.dtype),\n",
    "        len(ds.isna()),\n",
    "        len(data[data['sample'] == 1]) - len(ds.isna()),\n",
    "        round(np.min(ds[~ds.isna()]), 3),\n",
    "        round(np.max(ds[~ds.isna()]), 3),\n",
    "        round(np.mean(ds[~ds.isna()]), 3),\n",
    "        round(np.median(ds[~ds.isna()]), 3)\n",
    "    ]\n",
    "    values_log = [\n",
    "        '','','','',\n",
    "        round(np.min(dsl[~dsl.isna()]), 3),\n",
    "        round(np.max(dsl[~dsl.isna()]), 3),\n",
    "        round(np.mean(dsl[~dsl.isna()]), 3),\n",
    "        round(np.median(dsl[~dsl.isna()]), 3)\n",
    "    ]\n",
    "    trace4 = go.Table(header=dict(values=['', 'linear', 'log'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='center'),\n",
    "                    cells=dict(values=[sign_name, values, values_log],\n",
    "                    fill_color='lavender',\n",
    "                    align='center'))\n",
    "\n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(count_cuisine_linear, 1, 2)\n",
    "    fig.append_trace(trace2, 1, 3)\n",
    "    fig.append_trace(count_cuisine_log, 1, 4)\n",
    "    fig.append_trace(trace4, 1, 5)\n",
    "\n",
    "    fig.update_layout(title = 'Линейные значения и логарифм признака ' + column,\n",
    "                      title_x = 0.5,\n",
    "                      height=265, margin = dict(l=20, r=20, t=50, b=0), showlegend=False)\n",
    "\n",
    "    fig.show()\n",
    "    return data\n",
    "\n",
    "# Преобразование признака Ranking\n",
    "def ranking_distribution(data):\n",
    "    city_list = set(data['City'].to_list())\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    rank = pd.DataFrame()\n",
    "    for c in city_list:\n",
    "        df = data[data['City'] == c][['ID_TA', 'Ranking', 'sample']]\n",
    "        df['total_ranking'] = MinMaxScaler().fit_transform(np.array(df['Ranking']).reshape(-1,1))\n",
    "        df['standard_ranking'] = StandardScaler().fit_transform(np.array(df['Ranking']).reshape(-1,1))\n",
    "        rank = pd.concat([rank, df], sort=False)\n",
    "    rank.drop(['Ranking'], axis=1, inplace=True)\n",
    "    data = pd.merge(data, rank, on = ['ID_TA', 'sample'], how = 'left')\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признаков вокруг Ranking\")\n",
    "    return data\n",
    "\n",
    "def add_ranking_distribution(data):\n",
    "    mean_ranking_on_city = data.groupby(['City'])['Ranking'].mean()\n",
    "    data['mean_ranking_on_city'] = data['City'].apply(lambda x: mean_ranking_on_city[x])\n",
    "    data['norm_ranking_on_population'] = (data['Ranking'] - data['mean_ranking_on_city']) / (data['population_city'] / 1000)\n",
    "    data['norm_ranking_on_population'].astype('float64')\n",
    "    data['norm_ranking_on_tourists'] = (data['Ranking'] - data['mean_ranking_on_city']) / (data['count_city_tourists'] / data['population_city'])\n",
    "    data['norm_ranking_on_tourists'].astype('float64')\n",
    "    max_ranking_on_city = data.groupby(['City'])['Ranking'].max()\n",
    "    data['max_ranking_on_city'] = data['City'].apply(lambda x: max_ranking_on_city[x])\n",
    "    data['norm_ranking_on_max_rank'] = (data['Ranking'] - data['mean_ranking_on_city']) / data['max_ranking_on_city']\n",
    "    count_city_restaurant = data.groupby(['City'])['Ranking'].count()\n",
    "    data['count_city_restaurant'] = data['City'].apply(lambda x: count_city_restaurant[x])\n",
    "    data['norm_ranking_on_restaurant'] = (data['Ranking'] - data['mean_ranking_on_city']) / data['count_city_restaurant']\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления дополнительных признаков вокруг Ranking\")\n",
    "    return data\n",
    "\n",
    "# вывод распределения признака column по depth крупным позициям признака attribute\n",
    "def view_attribute_based_distribution(data, column, attribute, depth):\n",
    "    df = data[data['sample'] == 1]\n",
    "    layout = go.Layout(\n",
    "              autosize=False,\n",
    "              width=1200,\n",
    "              height=350)\n",
    "    fig = go.Figure(layout = layout)\n",
    "    for x in data[attribute].astype('str').value_counts()[0:depth].index:\n",
    "        fig.add_trace(go.Histogram(x = df[df[attribute].astype('str') == x][column], name = x, opacity = .75, nbinsx = 200))\n",
    "\n",
    "    fig.update_layout(title = 'Распределение признака ' + column + ' по ' + str(depth) + ' крупным значениям признака ' + attribute,\n",
    "                         title_x = 0.5,\n",
    "                         xaxis_title = 'Значение ' + column,\n",
    "                         yaxis_title = 'Количество',\n",
    "                         legend = dict(x = 1.05, y = 0.9,xanchor = 'center', orientation = 'v'),\n",
    "                         barmode = 'overlay',\n",
    "                         bargap=0.1,\n",
    "                         margin = dict(l=100, r=50, t=50, b=20))\n",
    "    fig.show()\n",
    "\n",
    "    # Преобразование Price Range\n",
    "def price_distribution(data, nan_value):\n",
    "    data['empty_price_range'] = pd.isna(data['Price Range']).astype('float64')\n",
    "    ds = data[data['sample'] == 1]['Price Range']\n",
    "    sign_name = ['строк', 'тип', 'значений', 'пропусков', 'min', 'max', 'mean',' median']\n",
    "    values = [\n",
    "        len(ds),\n",
    "        'object' if str(ds.dtype) == 'O' else str(ds.dtype),\n",
    "        len(ds) - len(ds[ds.isna()]),\n",
    "        len(ds[ds.isna()]),'','','',''\n",
    "    ]\n",
    "    print('{} значений признака пропущены в исходном датафрейме, это составляет {} % общей выборки'.\n",
    "        format(len(ds[ds.isna()]),\n",
    "        round(len(ds[ds.isna()]) / len(ds) * 100, 1)))\n",
    "\n",
    "    trace0 = go.Table(header=dict(values=['', 'исходные данные'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='center'),\n",
    "                    cells=dict(values=[sign_name, values],\n",
    "                    fill_color='lavender',\n",
    "                    align='center')) \n",
    "\n",
    "    # Перекодировка признака\n",
    "    price_dict = {'$':1, '$$ - $$$':2, '$$$$':3}\n",
    "    data['price_range'] = data['Price Range'].map(lambda x: price_dict[x] if x in price_dict else nan_value)\n",
    "    price_dict = {'$':'low', '$$ - $$$':'medium', '$$$$':'high'}\n",
    "    data['Price Range'] = data['Price Range'].map(lambda x: price_dict[x] if x in price_dict else np.nan)\n",
    "    ds = data[data['sample'] == 1]\n",
    "    ds0 = ds[ds['empty_price_range'] == 0]['price_range']\n",
    "    ds1 = ds[ds['empty_price_range'] == 1]['price_range']\n",
    "    \n",
    "    trace1_0 = go.Histogram(x = ds0, opacity=.65, name=\"исходные данные\", bingroup=0, marker_color='#0000CD')\n",
    "    trace1_1 = go.Histogram(x = ds1, opacity=.8, name=\"дополненные значения\", bingroup=0, marker_color='#FF0000')\n",
    "    \n",
    "    ds = ds['price_range']\n",
    "    values = [\n",
    "        len(ds),\n",
    "        'object' if str(ds.dtype) == 'O' else str(ds.dtype),\n",
    "        len(ds) - len(ds[ds.isna()]),\n",
    "        len(ds[ds.isna()]),\n",
    "        np.min(ds),\n",
    "        np.max(ds),\n",
    "        round(np.mean(ds), 3),\n",
    "        np.median(ds)\n",
    "    ]\n",
    "    trace2 = go.Table(header=dict(values=['', 'дополненные данные'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='center'),\n",
    "                    cells=dict(values=[sign_name, values],\n",
    "                    fill_color='lavender',\n",
    "                    align='center')) \n",
    "\n",
    "    fig = make_subplots(rows=1, cols=4, specs=[[{\"type\": \"table\"}, {\"type\": \"histogram\"}, {\"type\": \"table\"}, {\"type\": \"table\"}]])\n",
    "        \n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1_0, 1, 2)\n",
    "    fig.append_trace(trace1_1, 1, 2)\n",
    "    fig.append_trace(trace2, 1, 3)\n",
    "\n",
    "    fig.update_layout(barmode=\"stack\",\n",
    "                      bargap=0.1,\n",
    "                      legend = dict(xanchor = 'center', orientation = 'v'),\n",
    "                      height=225, margin = dict(l=20, r=20, t=0, b=0), showlegend=False)\n",
    "    fig.show()    \n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признаков вокруг Price Range\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# рассчет среднего чека по городам, вывод диаграммы\n",
    "def mean_price_in_city(data):\n",
    "    dict_price_in_city = data.groupby('City')['price_range'].mean().to_dict()\n",
    "    data['price_in_city'] = data['City'].map(dict_price_in_city)\n",
    "    data['price_in_city'] = MinMaxScaler().fit_transform(np.array(data['price_in_city']).reshape(-1,1))\n",
    "\n",
    "    dict_price_in_city = data.groupby('City')['price_in_city'].mean().to_dict()\n",
    "    revers_dict = dict(reversed(item) for item in dict_price_in_city.items())\n",
    "    X = sorted(revers_dict.keys(), reverse = True)\n",
    "    Y = [revers_dict[x] for x in X]\n",
    "    fig = go.Figure()\n",
    "\n",
    "    trace0 = go.Bar(y = Y, x = X, orientation='h')\n",
    "    fig.add_trace(trace0)\n",
    "    fig.update_layout(title = 'Средний уровень цен в городах (относительные значения)',\n",
    "                      title_x = 0.5,\n",
    "                      margin = dict(l=200, r=200, t=50, b=0))\n",
    "\n",
    "    fig.show()\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления среднего уровня цен в городах\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# вывод большой гистограммы распределения логарифма признака и таблицы с расчетом выбросов\n",
    "def view_histogram_n_outliers(data, column, how='all', bins=0):\n",
    "    ds = data[data['sample'] == 1][column]\n",
    "    ds = ds[~ds.isna()]\n",
    "    dsl = data[data['sample'] == 1]['log_'+column]\n",
    "    dsl = dsl[~dsl.isna()]\n",
    "\n",
    "    perc25_lin = ds.quantile(0.25)\n",
    "    perc75_lin = ds.quantile(0.75)\n",
    "    IQR_lin = perc75_lin - perc25_lin\n",
    "\n",
    "    perc25_log = dsl.quantile(0.25)\n",
    "    perc75_log = dsl.quantile(0.75)\n",
    "    IQR_log = perc75_log - perc25_log\n",
    "    \n",
    "    trace_lin_0 = go.Histogram(x = ds, name = 'все значения (lin)', nbinsx=bins, opacity=.75)\n",
    "    trace_lin_1 = go.Histogram(x = ds[(ds > perc75_lin + 1.5*IQR_lin) | (ds < (perc25_lin - 1.5*IQR_lin))], \n",
    "                               name = 'выбросы (lin)', nbinsx=int(bins/4), opacity=.5)\n",
    "    trace_log_0 = go.Histogram(x = dsl[dsl <= perc75_log + 1.5*IQR_log], name = 'все значения (log)', nbinsx=int(bins/2), opacity=.75)\n",
    "    trace_log_1 = go.Histogram(x = dsl[(dsl > perc75_log + 1.5*IQR_log) | (dsl < (perc25_log - 1.5*IQR_log))], \n",
    "                               name = 'выбросы (log)', nbinsx=int(bins/8), opacity=.5)\n",
    "    \n",
    "    if how == 'all':\n",
    "        fig = make_subplots(rows=2, cols=2, column_widths=[1100,500], row_heights=[200,200], \n",
    "                        specs=[[{\"type\": \"histogram\"}, {\"type\": \"table\", 'rowspan': 2}],\n",
    "                              [{\"type\": \"histogram\"}, None]])\n",
    "        fig.add_trace(trace_lin_0,1,1)\n",
    "        fig.add_trace(trace_lin_1,1,1)\n",
    "\n",
    "        fig.add_trace(trace_log_0,2,1)\n",
    "        fig.add_trace(trace_log_1,2,1)\n",
    "\n",
    "    else:\n",
    "        fig = make_subplots(rows=1, cols=2, column_widths=[1100,500], row_heights=[300], \n",
    "                        specs=[[{\"type\": \"histogram\"}, {\"type\": \"table\"}]])\n",
    "        if how == 'lin':\n",
    "            fig.add_trace(trace_lin_0,1,1)\n",
    "            fig.add_trace(trace_lin_1,1,1)            \n",
    "        elif how == 'log':\n",
    "            fig.add_trace(trace_log_0,1,1)\n",
    "            fig.add_trace(trace_log_1,1,1)\n",
    "\n",
    "    sign_name = ['первый квантиль', 'медиана', 'третий квантиль', 'межквантильный диапазон', 'нижняя граница выбросов', \n",
    "                 'верхняя граница выбросов', 'кол-во значений за нижней границей','кол-во значений за верхней границей']\n",
    "    values = [\n",
    "            perc25_lin,\n",
    "            np.median(ds),\n",
    "            perc75_lin,\n",
    "            IQR_lin,\n",
    "            round(perc25_lin - 1.5*IQR_lin, 3),\n",
    "            round(perc75_lin + 1.5*IQR_lin, 3),\n",
    "            len(ds[(ds < perc25_lin - 1.5*IQR_lin)]),\n",
    "            len(ds[(ds > perc75_lin + 1.5*IQR_lin)])\n",
    "        ]\n",
    "    values_log = [\n",
    "            round(perc25_log, 3),\n",
    "            round(np.median(dsl), 3),\n",
    "            round(perc75_log, 3),\n",
    "            round(IQR_log, 3),\n",
    "            round(perc25_log - 1.5*IQR_log, 3),\n",
    "            round(perc75_log + 1.5*IQR_log, 3),\n",
    "            len(dsl[(dsl < perc25_log - 1.5*IQR_log)]),\n",
    "            len(dsl[(dsl > perc75_log + 1.5*IQR_log)])\n",
    "        ]\n",
    "    fig.add_trace(go.Table(header=dict(values=['<b>Параметр</b>', '<b>lin</b>', '<b>log</b>'],\n",
    "                        fill_color='paleturquoise',\n",
    "                        align='center',\n",
    "                        height=40),\n",
    "                        cells=dict(values=[sign_name, values, values_log],\n",
    "                        fill_color='lavender',\n",
    "                        align=['left']+['center']*2,\n",
    "                        height=50),\n",
    "                        columnwidth = [250,100,100],),1,2)\n",
    "\n",
    "\n",
    "    fig.update_layout(title = 'Исследование распределения и выбросов признака ' + column,\n",
    "                     title_x = 0.5,\n",
    "                     legend = dict(x=.5, y=.9, xanchor = 'center', orientation = 'v'),\n",
    "                     barmode = 'overlay',\n",
    "                     bargap=0.1,\n",
    "                     margin = dict(l=50, r=50, t=50, b=0))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# обработка текстовой части признака Reviews. Подсчет количества слов с позитивной и негативной окраской\n",
    "def review_text_distribution(data):\n",
    "    pattern_max = re.compile('((?!and)(?!for)(?!the)[A-Z|a-z]{3,})')\n",
    "    data['review_words'] = data['Reviews'].apply(lambda x: pattern_max.findall(str(x).lower()))\n",
    "    data['count_review_words'] = data['review_words'].apply(lambda x: len(x))\n",
    "    \n",
    "    data['Reviews'] = data['Reviews'].fillna('[[], []]')\n",
    "    data['empty_review'] = data['Reviews'].apply(lambda x: 1 if x == '[[], []]' else 0)\n",
    "    data['review_words'] = data.apply(lambda x: 'empty' if x['empty_review'] == 1 else x['review_words'], axis=1)\n",
    "    \n",
    "    data = string_to_list_distribution(data, 'review_words')\n",
    "\n",
    "    positive_words, negative_words = read_positive_words()\n",
    "\n",
    "    data['count_pos_words'] = data['list_review_words'].apply(lambda x: len(set(x)&set(positive_words)))\n",
    "    data['count_neg_words'] = data['list_review_words'].apply(lambda x: len(set(x)&set(negative_words)))\n",
    "    data['count_neg_words'] = data.apply(lambda x: x['count_neg_words']-1 if 'not' in x['list_review_words'] else x['count_neg_words'], axis = 1)\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признаков вокруг текста признака Reviews\")\n",
    "    return data\n",
    "\n",
    "def read_positive_words():\n",
    "    # читаем списки позитивных и негативных слов\n",
    "    DATA_DIR = '/kaggle/input/opinion-lexicon-english/'\n",
    "    positive_words = pd.read_csv(DATA_DIR +'positive-words.txt', skiprows=34, names=['word'])\n",
    "    positive_words = positive_words['word'].to_list()\n",
    "    negative_words = pd.read_csv(DATA_DIR +'neg_words.txt', 'r', encoding=\"ISO-8859-1\", names=['word'])\n",
    "    negative_words = list(set(negative_words['word'].to_list()))\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "# работа с датами в признаке Reviews\n",
    "def data_review_distribution(data):\n",
    "    pattern_max = re.compile('\\[\\'(\\d{2}\\/\\d{2}/\\\\d{4})')\n",
    "    pattern_min = re.compile('(\\d{2}\\/\\d{2}/\\\\d{4})\\'\\]')\n",
    "    data['Review_date_max'] = data['Reviews'].apply(lambda x: str(pattern_max.findall(str(x))))\n",
    "    data['Review_date_min'] = data['Reviews'].apply(lambda x: str(pattern_min.findall(str(x))))\n",
    "    data['review_date_count'] = data.apply(lambda x: 0 if x['Reviews'] == '[[], []]' else \n",
    "                                           1 if x['Review_date_max'] == x['Review_date_min'] else 2, axis=1)\n",
    "    # Преобразуем даты в дни\n",
    "    data['Review_date_max'] = (pd.datetime.now() - pd.to_datetime(data['Review_date_max'], format=\"['%m/%d/%Y']\", errors='coerce')).dt.days\n",
    "    data['Review_date_min'] = (pd.datetime.now() - pd.to_datetime(data['Review_date_min'], format=\"['%m/%d/%Y']\", errors='coerce')).dt.days\n",
    "    data['review_date_min'] = data['Review_date_min'].apply(lambda x: x - data['Review_date_min'].min() + 1)\n",
    "    data['review_date_delta'] = data['Review_date_min'] - data['Review_date_max']\n",
    "    data['review_date_delta'] = data['review_date_delta'].apply(lambda x: abs(x))\n",
    "    # добавляем признак сезона\n",
    "    #data['review_date_season'] = data['Review_date_min'].apply(lambda x: season_from_year(x))\n",
    "    \n",
    "    del data['Review_date_max']\n",
    "    del data['Review_date_min']\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), \"После добавления признаков вокруг календарных переменных Reviews\")    \n",
    "    return data\n",
    "\n",
    "def show_heatmap(df):\n",
    "    corrs = df.corr()\n",
    "    fig = ff.create_annotated_heatmap(z=corrs.values,\n",
    "                                    x=list(corrs.columns),\n",
    "                                    y=list(corrs.index),\n",
    "                                    opacity=.8,\n",
    "                                    annotation_text=corrs.round(2).values,\n",
    "                                    showscale=True)\n",
    "    fig.update_layout(#title = 'Тепловая карта матрицы корреляций',\n",
    "                     title_x = 0.5,\n",
    "                     legend = dict(x = .5, xanchor = 'center', orientation = 'h'),\n",
    "                     margin = dict(l=100, r=100, t=0, b=0))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# стандартизация\n",
    "def normalisation(df, scaler, not_norm = [], columns_list='all'):\n",
    "    if columns_list == 'all':\n",
    "        columns_list = df.columns.tolist()\n",
    "    for column in columns_list:\n",
    "        if df[column].dtype in ['float64', 'int64'] and (column not in not_norm):\n",
    "            df[column] = scaler.fit_transform(np.array(df[column]).reshape(-1,1))\n",
    "    return df\n",
    "\n",
    "# удаление нечисловых признаков\n",
    "def delete_string_sign(df):\n",
    "    for column in df.columns.tolist():\n",
    "        if df[column].dtype == 'object':\n",
    "            del df[column]\n",
    "    return df\n",
    "\n",
    "# применение метода главных компонент\n",
    "def pca_distribution(data, list_for_pca, pca_name, list_for_save=[]):\n",
    "    if len(list_for_save) == 0: list_for_save = [False for i in range(len(list_for_pca))] \n",
    "    data_temp = data.loc[data['sample'] == 1]\n",
    "    for column in list_for_pca:\n",
    "        data_temp[column] = MinMaxScaler().fit_transform(np.array(data_temp[column]).reshape(-1,1))\n",
    "    matrix = np.array(data_temp[list_for_pca].corr())\n",
    "    eig_num, eig_v = np.linalg.eig(matrix)\n",
    "    res = np.zeros(len(data))\n",
    "    for i in range(len(list_for_pca)):\n",
    "        res = res + np.array(data[list_for_pca[i]])*eig_v.T[0][i]    \n",
    "    data[pca_name] = res\n",
    "    list_del = []\n",
    "    for i in range(len(list_for_pca)):\n",
    "        if list_for_save[i] == False or list_for_save[i] == 0:\n",
    "            list_del.append(list_for_pca[i])\n",
    "    data.drop(list_del, axis=1, inplace=True )\n",
    "    text = (\"\\nПризнаки {} по методу главных компонент преобразованы в признак '{}'. \\nВектор главных компонент: {}\"\n",
    "            \"\\nПосле преобразования\".\n",
    "            format(str(list_for_pca)[1:-1], pca_name, list(eig_v.T[0])))\n",
    "    print_report(len(data), len(data[data['sample']==1]), len(data.columns), text)\n",
    "    return data\n",
    "\n",
    "def read_dataframes():\n",
    "    DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\n",
    "    df_train = pd.read_csv(DATA_DIR + '/main_task.csv')\n",
    "    df_test = pd.read_csv(DATA_DIR + 'kaggle_task.csv')\n",
    "    sample_submission = pd.read_csv(DATA_DIR + '/sample_submission.csv')\n",
    "\n",
    "    # ВАЖНО! для корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "    df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "    df_test['sample'] = 0 # помечаем где у нас тест\n",
    "    df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "    data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n",
    "    return data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
